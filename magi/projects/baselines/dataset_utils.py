from acme import types
import d4rl  # type: ignore
import gym
import numpy as np
from tqdm import tqdm


def split_into_trajectories(observations, actions, rewards, masks, dones_float,
                            next_observations):
  trajs = [[]]

  for i in tqdm(range(len(observations))):
    trajs[-1].append((
        observations[i],
        actions[i],
        rewards[i],
        masks[i],
        dones_float[i],
        next_observations[i],
    ))
    if dones_float[i] == 1.0 and i + 1 < len(observations):
      trajs.append([])

  return trajs


def merge_trajectories(trajs):
  observations = []
  actions = []
  rewards = []
  masks = []
  dones_float = []
  next_observations = []

  for traj in trajs:
    for (obs, act, rew, mask, done, next_obs) in traj:
      observations.append(obs)
      actions.append(act)
      rewards.append(rew)
      masks.append(mask)
      dones_float.append(done)
      next_observations.append(next_obs)

  return (
      np.stack(observations),
      np.stack(actions),
      np.stack(rewards),
      np.stack(masks),
      np.stack(dones_float),
      np.stack(next_observations),
  )


class Dataset(object):

  def __init__(
      self,
      observations: np.ndarray,
      actions: np.ndarray,
      rewards: np.ndarray,
      masks: np.ndarray,
      dones_float: np.ndarray,
      next_observations: np.ndarray,
      size: int,
  ):
    self.observations = observations
    self.actions = actions
    self.rewards = rewards
    self.masks = masks
    self.dones_float = dones_float
    self.next_observations = next_observations
    self.size = size

  def sample(self, batch_size: int) -> types.Transition:
    indx = np.random.randint(self.size, size=batch_size)
    return types.Transition(
        observation=self.observations[indx],
        action=self.actions[indx],
        reward=self.rewards[indx],
        discount=self.masks[indx],
        next_observation=self.next_observations[indx],
    )


class D4RLDataset(Dataset):

  def __init__(self, env: gym.Env, clip_to_eps: bool = True, eps: float = 1e-5):
    dataset = d4rl.qlearning_dataset(env)

    if clip_to_eps:
      lim = 1 - eps
      dataset['actions'] = np.clip(dataset['actions'], -lim, lim)

    dones_float = np.zeros_like(dataset['rewards'])

    for i in range(len(dones_float) - 1):
      if (np.linalg.norm(dataset['observations'][i + 1] -
                         dataset['next_observations'][i]) > 1e-6 or
          dataset['terminals'][i] == 1.0):
        dones_float[i] = 1
      else:
        dones_float[i] = 0

    dones_float[-1] = 1

    super().__init__(
        dataset['observations'].astype(np.float32),
        actions=dataset['actions'].astype(np.float32),
        rewards=dataset['rewards'].astype(np.float32),
        masks=1.0 - dataset['terminals'].astype(np.float32),
        dones_float=dones_float.astype(np.float32),
        next_observations=dataset['next_observations'].astype(np.float32),
        size=len(dataset['observations']),
    )
