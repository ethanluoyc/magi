"""SAC builder"""
from typing import Iterator, List, Optional

from acme import adders
from acme import core
from acme import datasets
from acme import specs
from acme.adders import reverb as adders_reverb
from acme.jax import networks as networks_lib
from acme.jax import variable_utils
from acme.jax.types import Networks  # pylint: disable=g-multiple-import
from acme.jax.types import PolicyNetwork
from acme.jax.types import Sample
from acme.utils import counting
from acme.utils import loggers
import optax
import reverb
from reverb import rate_limiters

from magi.agents.sac import acting as acting_lib
from magi.agents.sac import config as sac_config
from magi.agents.sac import learning as learning_lib


class SACBuilder:
    """Soft Actor-Critic agent specification"""

    def __init__(self, config: sac_config.SACConfig, networks: Networks):
        self._config = config
        self._networks = networks

    def make_replay_tables(
        self,
        environment_spec: specs.EnvironmentSpec,
    ) -> List[reverb.Table]:
        """Create tables to insert data into."""
        replay_table = reverb.Table(
            name=self._config.replay_table_name,
            # TODO(yl): support prioritized sampling in SAC
            sampler=reverb.selectors.Uniform(),
            remover=reverb.selectors.Fifo(),
            max_size=self._config.max_replay_size,
            rate_limiter=rate_limiters.MinSize(self._config.min_replay_size),
            signature=adders_reverb.NStepTransitionAdder.signature(
                environment_spec=environment_spec
            ),
        )
        return [replay_table]

    def make_dataset_iterator(
        self,
        replay_client: reverb.Client,
    ) -> Iterator[Sample]:
        """Create a dataset iterator to use for learning/updating the agent."""
        dataset = datasets.make_reverb_dataset(
            table=self._config.replay_table_name,
            server_address=replay_client.server_address,
            batch_size=self._config.batch_size,
            prefetch_size=self._config.prefetch_size,
            transition_adder=True,
        )
        return dataset.as_numpy_iterator()

    def make_adder(
        self,
        replay_client: reverb.Client,
    ) -> Optional[adders.Adder]:
        """Create an adder which records data generated by the actor/environment.

        Args:
          replay_client: Reverb Client which points to the replay server.
        """
        # TODO(yl): support multi step transitions
        return adders_reverb.NStepTransitionAdder(
            client=replay_client, n_step=1, discount=1.0
        )

    def make_actor(
        self,
        random_key: networks_lib.PRNGKey,
        policy_network: PolicyNetwork,
        adder: Optional[adders.Adder] = None,
        variable_source: Optional[core.VariableSource] = None,
        is_eval: bool = False,
    ) -> core.Actor:
        """Create an actor instance.

        Args:
          random_key: A key for random number generation.
          policy_network: Instance of a policy network; this should be a callable
            which takes as input observations and returns actions.
          adder: How data is recorded (e.g. added to replay).
          variable_source: A source providing the necessary actor parameters.
        """
        assert variable_source is not None
        variable_client = variable_utils.VariableClient(variable_source, "")
        variable_client.update_and_wait()
        return acting_lib.SACActor(
            policy_network.apply,
            random_key,
            is_eval=is_eval,
            variable_client=variable_client,
            adder=adder,
        )

    def make_learner(
        self,
        environment_spec: specs.EnvironmentSpec,
        random_key: networks_lib.PRNGKey,
        networks: Networks,
        dataset: Iterator[Sample],
        logger: Optional[loggers.Logger] = None,
        counter: Optional[counting.Counter] = None,
    ) -> core.Learner:
        """Creates an instance of the learner.

        Args:
          random_key: A key for random number generation.
          networks: struct describing the networks needed by the learner; this can
            be specific to the learner in question.
          dataset: iterator over samples from replay.
          replay_client: client which allows communication with replay, e.g. in
            order to update priorities.
          counter: a Counter which allows for recording of counts (learner steps,
            actor steps, etc.) distributed throughout the agent.
          checkpoint: bool controlling whether the learner checkpoints itself.
        """
        config = self._config
        critic_opt = optax.chain(
            optax.clip_by_global_norm(config.max_gradient_norm),
            optax.adam(config.critic_learning_rate),
        )
        actor_opt = optax.chain(
            optax.clip_by_global_norm(config.max_gradient_norm),
            optax.adam(config.actor_learning_rate),
        )
        alpha_opt = optax.chain(
            optax.clip_by_global_norm(config.max_gradient_norm),
            optax.adam(config.temperature_learning_rate, b1=config.temperature_adam_b1),
        )

        self._learner = learning_lib.SACLearner(
            environment_spec,
            networks["policy"],
            networks["critic"],
            key=random_key,
            dataset=dataset,
            actor_optimizer=actor_opt,
            critic_optimizer=critic_opt,
            alpha_optimizer=alpha_opt,
            gamma=config.discount,
            tau=config.critic_soft_update_rate,
            init_alpha=config.init_temperature,
            logger=logger,
            counter=counter,
        )
