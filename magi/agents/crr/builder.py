from typing import Any, Callable, Iterator, List, Optional

from acme import adders
from acme import core
from acme import specs
from acme.adders import reverb as adders_reverb
from acme.agents.jax import actor_core as actor_core_lib
from acme.agents.jax import actors
from acme.agents.jax import builders
from acme.datasets import reverb as datasets
from acme.jax import networks as networks_lib
from acme.jax import variable_utils
from acme.utils import counting
from acme.utils import loggers
import reverb
from reverb import rate_limiters

from magi.agents.crr import config as crr_config
from magi.agents.crr import learning

PolicyNetwork = Any


class CRRBuilder(builders.ActorLearnerBuilder):
    """Soft Actor-Critic agent specification"""

    def __init__(
        self,
        config: crr_config.CRRConfig,
        logger_fn: Callable[[], loggers.Logger] = lambda: None,
    ):
        self._config = config
        self._logger_fn = logger_fn

    def make_replay_tables(
        self,
        environment_spec: specs.EnvironmentSpec,
    ) -> List[reverb.Table]:
        """Create tables to insert data into."""
        replay_table = reverb.Table(
            name=self._config.replay_table_name,
            # TODO(yl): support prioritized sampling
            sampler=reverb.selectors.Uniform(),
            remover=reverb.selectors.Fifo(),
            max_size=self._config.max_replay_size,
            rate_limiter=rate_limiters.MinSize(self._config.min_replay_size),
            signature=adders_reverb.NStepTransitionAdder.signature(
                environment_spec=environment_spec
            ),
        )
        return [replay_table]

    def make_dataset_iterator(
        self,
        replay_client: reverb.Client,
    ) -> Iterator[reverb.ReplaySample]:
        """Create a dataset iterator to use for learning/updating the agent."""
        dataset = datasets.make_reverb_dataset(
            table=self._config.replay_table_name,
            server_address=replay_client.server_address,
            batch_size=self._config.batch_size,
            transition_adder=True,
        )
        return dataset.as_numpy_iterator()

    def make_adder(
        self,
        replay_client: reverb.Client,
    ) -> Optional[adders.Adder]:
        """Create an adder which records data generated by the actor/environment.

        Args:
          replay_client: Reverb Client which points to the replay server.
        """
        # TODO(yl): support multi step transitions
        return adders_reverb.NStepTransitionAdder(
            client=replay_client, n_step=1, discount=self._config.discount
        )

    def make_actor(
        self,
        random_key: networks_lib.PRNGKey,
        policy_network: PolicyNetwork,
        adder: Optional[adders.Adder] = None,
        variable_source: Optional[core.VariableSource] = None,
    ) -> core.Actor:
        """Create an actor instance."""
        assert variable_source is not None
        variable_client = variable_utils.VariableClient(variable_source, "policy")
        variable_client.update_and_wait()
        actor_core = actor_core_lib.batched_feed_forward_to_actor_core(policy_network)

        return actors.GenericActor(
            actor_core, random_key, variable_client=variable_client, adder=adder
        )

    def make_learner(
        self,
        random_key: networks_lib.PRNGKey,
        networks,
        dataset: Iterator[reverb.ReplaySample],
        replay_client: Optional[reverb.Client] = None,
        counter: Optional[counting.Counter] = None,
    ):
        del replay_client
        learner = learning.CRRLearner(
            policy_network=networks["policy"],
            critic_network=networks["critic"],
            policy_optimizer=self._config.policy_optimizer,
            critic_optimizer=self._config.critic_optimizer,
            random_key=random_key,
            dataset=dataset,
            discount=self._config.discount,
            target_update_period=self._config.target_update_period,
            num_action_samples_td_learning=self._config.num_action_samples_td_learning,
            num_action_samples_policy_weight=self._config.num_action_samples_policy_weight,
            baseline_reduce_function=self._config.baseline_reduce_function,
            policy_improvement_modes=self._config.policy_improvement_modes,
            ratio_upper_bound=self._config.ratio_upper_bound,
            beta=self._config.beta,
            logger=self._logger_fn(),
            counter=counter,
        )
        return learner
