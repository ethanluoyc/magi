"""TD3 agent implementation"""
import dataclasses
from typing import Iterator, List, Optional, Sequence

import acme
from acme import adders
from acme import core
from acme import datasets
from acme import specs
from acme import types
from acme.adders import reverb as adders_reverb
from acme.agents.jax import actors as acting_lib
from acme.jax import networks as networks_lib
from acme.jax import types as jax_types
from acme.jax import variable_utils
from acme.utils import counting
from acme.utils import loggers
import dm_env
import jax
import jax.numpy as jnp
import optax
import reverb
from reverb import rate_limiters

from magi.agents import actors as random_acting
from magi.agents.td3 import learning as learning_lib


@dataclasses.dataclass
class TD3Config:
    """Configuration for TD3 agent."""

    # Minimum size for the replay buffer.
    # This also determines the number of environment steps to take
    # before calling `learner.step`
    min_replay_size: int = 1_000
    # Capacity of the replay buffer.
    max_replay_size: int = 1_000_000
    # Reverb replay table name.
    replay_table_name: str = adders_reverb.DEFAULT_PRIORITY_TABLE

    # Prefetch size from Reverb replay
    prefetch_size: Optional[int] = None
    # Batch size for sampling transitions
    batch_size: int = 256

    # Optimizer for the policy
    policy_optimizer: Optional[optax.GradientTransformation] = None
    # Optimizer for the critic
    critic_optimizer: Optional[optax.GradientTransformation] = None

    # Additional discounts (usually referred to as gamma in the literature)
    discount: float = 0.99
    # Rate for updating the target parameters by Polyak averaging (tau)
    soft_update_rate: float = 0.005
    # Noise added to the actions used by the critic
    policy_noise: float = 0.2
    # Clipping parameter for the noise
    policy_noise_clip: float = 0.5
    # Exploration noise for the behavior policy
    policy_exploration_noise: float = 0.1
    # Number of actor steps for every policy and target network update
    policy_update_period: int = 2


class TD3Builder:
    """Builder for creating TD3 agent."""

    def __init__(self, config: TD3Config):
        self._config = config

    def make_replay_tables(
        self,
        environment_spec: specs.EnvironmentSpec,
    ) -> List[reverb.Table]:
        """Create tables to insert data into."""
        replay_table = reverb.Table(
            name=self._config.replay_table_name,
            # TODO(yl): support prioritized sampling in SAC
            sampler=reverb.selectors.Uniform(),
            remover=reverb.selectors.Fifo(),
            max_size=self._config.max_replay_size,
            rate_limiter=rate_limiters.MinSize(self._config.min_replay_size),
            signature=adders_reverb.NStepTransitionAdder.signature(
                environment_spec=environment_spec
            ),
        )
        return [replay_table]

    def make_dataset_iterator(
        self,
        replay_client: reverb.Client,
    ) -> Iterator[reverb.ReplaySample]:
        """Create a dataset iterator to use for learning/updating the agent."""
        dataset = datasets.make_reverb_dataset(
            table=self._config.replay_table_name,
            server_address=replay_client.server_address,
            batch_size=self._config.batch_size,
            prefetch_size=self._config.prefetch_size,
            transition_adder=True,
        )
        return dataset.as_numpy_iterator()

    def make_adder(
        self,
        replay_client: reverb.Client,
    ) -> Optional[adders.Adder]:
        """Create an adder which records data generated by the actor/environment.

        Args:
          replay_client: Reverb Client which points to the replay server.
        """
        # TODO(yl): support multi step transitions
        return adders_reverb.NStepTransitionAdder(
            client=replay_client, n_step=1, discount=self._config.discount
        )

    def make_actor(
        self,
        random_key: jax_types.PRNGKey,
        policy_network: networks_lib.FeedForwardNetwork,
        adder: Optional[adders.Adder] = None,
        variable_source: Optional[core.VariableSource] = None,
    ) -> core.Actor:
        """Create an actor instance.

        Args:
          random_key: A key for random number generation.
          policy_network: Instance of a policy network; this should be a callable
            which takes as input observations and returns actions.
          adder: How data is recorded (e.g. added to replay).
          variable_source: A source providing the necessary actor parameters.
        """
        assert variable_source is not None
        variable_client = variable_utils.VariableClient(variable_source, "")
        variable_client.update_and_wait()
        return acting_lib.FeedForwardActor(
            policy_network, random_key, variable_client, adder
        )

    def make_learner(
        self,
        environment_spec: specs.EnvironmentSpec,
        random_key: jax_types.PRNGKey,
        networks: networks_lib.FeedForwardNetwork,
        dataset: Iterator[reverb.ReplaySample],
        logger: Optional[loggers.Logger] = None,
        counter: Optional[counting.Counter] = None,
    ) -> core.Learner:
        return learning_lib.TD3Learner(
            environment_spec,
            policy_network=networks["policy"],
            critic_network=networks["critic"],
            iterator=dataset,
            random_key=random_key,
            policy_optimizer=self._config.policy_optimizer,
            critic_optimizer=self._config.critic_optimizer,
            discount=self._config.discount,
            soft_update_rate=self._config.soft_update_rate,
            policy_noise=self._config.policy_noise,
            policy_noise_clip=self._config.policy_noise_clip,
            policy_target_update_period=self._config.policy_update_period,
            logger=logger,
            counter=counter,
        )


class TD3Agent(acme.Actor, acme.VariableSource):
    """Single-process TD3 agent implementation.

    References:
        [1]: Fujimoto, Scott and Hoof, Herke and Meger, David.
             Addressing Function Approximation Error in Actor-Critic Methods.
             In _International Conference on Machine Learning_, 2018.
             https://arxiv.org/abs/1802.09477
    """

    builder: TD3Builder

    def __init__(
        self,
        environment_spec: specs.EnvironmentSpec,
        policy_network: networks_lib.FeedForwardNetwork,
        critic_network: networks_lib.FeedForwardNetwork,
        config: TD3Config,
        random_key: jax_types.PRNGKey,
        logger: Optional[loggers.Logger] = None,
        counter: Optional[counting.Counter] = None,
    ):
        builder = TD3Builder(config)

        # Create the replay server and grab its address.
        replay_tables = builder.make_replay_tables(environment_spec)
        replay_server = reverb.Server(replay_tables, port=None)
        replay_client = reverb.Client(f"localhost:{replay_server.port}")

        network_spec = {
            "policy": policy_network,
            "critic": critic_network,
        }

        # Create actor, dataset, and learner for generating, storing, and consuming
        # data respectively.
        learner_key, actor_key, random_actor_key = jax.random.split(random_key, 3)

        def behavior_policy(params, key, observation):
            action_spec = environment_spec.actions
            action_mean = policy_network.apply(params, observation)
            exploration_noise = (
                jax.random.normal(key, action_mean.shape)
                * config.policy_exploration_noise
            )
            sampled_action = action_mean + exploration_noise
            sampled_action = jnp.clip(
                sampled_action, action_spec.minimum, action_spec.maximum
            )
            return sampled_action

        dataset = builder.make_dataset_iterator(replay_client)
        learner = builder.make_learner(
            environment_spec,
            learner_key,
            network_spec,
            dataset,
            counter=counter,
            logger=logger,
        )
        adder = builder.make_adder(replay_client)
        actor = builder.make_actor(actor_key, behavior_policy, adder, learner)

        # Internalize agent components
        self.builder = builder
        self._actor = actor
        self._learner = learner
        self._replay_server = replay_server
        # TODO(yl): It is not clear if we need to use the random actor
        # for the initial exploration. Consider benchmarking against with using
        # the actor with randomly initialized parameters
        self._random_actor = random_acting.RandomActor(
            environment_spec.actions, random_actor_key, adder=adder
        )
        self._num_observations = 0
        self._min_observations = config.min_replay_size

    def _get_active_actor(self) -> acme.Actor:
        if self._num_observations < self._min_observations:
            return self._random_actor
        else:
            return self._actor

    def select_action(self, observation: types.NestedArray):
        return self._get_active_actor().select_action(observation)

    def observe_first(self, timestep: dm_env.TimeStep):
        return self._get_active_actor().observe_first(timestep)

    def observe(self, action: types.NestedArray, next_timestep: dm_env.TimeStep):
        self._num_observations += 1
        self._get_active_actor().observe(action, next_timestep)

    def update(self):
        if self._num_observations < self._min_observations:
            return
        self._learner.step()
        self._actor.update(wait=True)

    def get_variables(self, names: Sequence[str]) -> List[types.NestedArray]:
        return self._learner.get_variables(names)
