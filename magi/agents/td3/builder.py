"""TD3 builder"""
from typing import Callable, Dict, Iterator, List, Optional

from acme import adders
from acme import core
from acme import datasets
from acme import specs
from acme.adders import reverb as adders_reverb
from acme.agents.jax import actor_core
from acme.agents.jax import actors as acting_lib
from acme.agents.jax import builders
from acme.jax import networks as networks_lib
from acme.jax import types as jax_types
from acme.jax import variable_utils
from acme.utils import counting
from acme.utils import loggers
import reverb

from magi.agents.td3 import config as td3_config
from magi.agents.td3 import learning as learning_lib


class TD3Builder(builders.ActorLearnerBuilder):
    """Builder for creating TD3 agent."""

    def __init__(
        self,
        config: td3_config.TD3Config,
        logger_fn: Callable[[], loggers.Logger],
    ):
        self._config = config
        self._logger_fn = logger_fn

    def make_replay_tables(
        self,
        environment_spec: specs.EnvironmentSpec,
    ) -> List[reverb.Table]:
        """Create tables to insert data into."""
        replay_table = reverb.Table(
            name=self._config.replay_table_name,
            # TODO(yl): support prioritized sampling in SAC
            sampler=reverb.selectors.Uniform(),
            remover=reverb.selectors.Fifo(),
            max_size=self._config.max_replay_size,
            rate_limiter=reverb.rate_limiters.MinSize(self._config.min_replay_size),
            signature=adders_reverb.NStepTransitionAdder.signature(
                environment_spec=environment_spec
            ),
        )
        return [replay_table]

    def make_dataset_iterator(
        self,
        replay_client: reverb.Client,
    ) -> Iterator[reverb.ReplaySample]:
        """Create a dataset iterator to use for learning/updating the agent."""
        dataset = datasets.make_reverb_dataset(
            table=self._config.replay_table_name,
            server_address=replay_client.server_address,
            batch_size=self._config.batch_size,
            prefetch_size=self._config.prefetch_size,
            transition_adder=True,
        )
        return dataset.as_numpy_iterator()

    def make_adder(
        self,
        replay_client: reverb.Client,
    ) -> Optional[adders.Adder]:
        """Create an adder which records data generated by the actor/environment.

        Args:
          replay_client: Reverb Client which points to the replay server.
        """
        # TODO(yl): support multi step transitions
        return adders_reverb.NStepTransitionAdder(
            client=replay_client, n_step=1, discount=self._config.discount
        )

    def make_actor(
        self,
        random_key: jax_types.PRNGKey,
        policy_network: networks_lib.FeedForwardNetwork,
        adder: Optional[adders.Adder] = None,
        variable_source: Optional[core.VariableSource] = None,
    ) -> core.Actor:
        """Create an actor instance.

        Args:
          random_key: A key for random number generation.
          policy_network: Instance of a policy network; this should be a callable
            which takes as input observations and returns actions.
          adder: How data is recorded (e.g. added to replay).
          variable_source: A source providing the necessary actor parameters.
        """
        assert variable_source is not None
        variable_client = variable_utils.VariableClient(variable_source, "policy")
        variable_client.update_and_wait()
        return acting_lib.GenericActor(
            actor=actor_core.batched_feed_forward_to_actor_core(policy_network),
            random_key=random_key,
            variable_client=variable_client,
            adder=adder,
        )

    def make_learner(
        self,
        random_key: jax_types.PRNGKey,
        networks: Dict[str, networks_lib.FeedForwardNetwork],
        dataset: Iterator[reverb.ReplaySample],
        counter: Optional[counting.Counter] = None,
    ) -> core.Learner:
        return learning_lib.TD3Learner(
            policy_network=networks["policy"],
            critic_network=networks["critic"],
            iterator=dataset,
            random_key=random_key,
            policy_optimizer=self._config.policy_optimizer,
            critic_optimizer=self._config.critic_optimizer,
            discount=self._config.discount,
            soft_update_rate=self._config.soft_update_rate,
            policy_noise=self._config.policy_noise,
            policy_noise_clip=self._config.policy_noise_clip,
            policy_target_update_period=self._config.policy_update_period,
            logger=self._logger_fn(),
            counter=counter,
        )
